#+TITLE: Nova Scheduler
#+DATE: 2018-04-26
#+AUTHOR: Andrey Volkov
#+EMAIL: avolkov@mirantis.com
#+OPTIONS: ^:nil
#+OPTIONS: f:t
#+OPTIONS: \n:t

* Nova Scheduler
  :PROPERTIES:
  :header-args: :eval never-export
  :END:

** Intro

This doc contains 3 parts describing the following:

- common Nova scheduler principles related to scheduler setup, scheduler logic of picking a host and
  params affecting the scheduling process;
- information about Mirantis analyzer tool for Nova scheduling;
- log of booting VMs on Mirantis test lab.

** Nova Scheduler

*** Scheduler Setup

Nova scheduling process is configurable. First of all, you can choose what driver for scheduling is
used in *CONF/scheduler/driver* [fn:1]. Several options are available e.g. filter_scheduler,
caching_scheduler or chance_scheduler. As the filter_scheduler is used by default, in this doc the
most attention is focused on it. Next important setting is *CONF/scheduler/host_manager* [fn:2]
that represents the in-memory picture of the hosts and their properties that scheduler takes into
account.

For the filter_scheduler important settings are *CONF/filter_scheduler/available_filters* which are
links to python objects loaded into memory and could be used in
*CONF/filter_scheduler/enabled_filters* that are *really* used for filtering.

*CONF/filter_scheduler/weight_classes* is a list of python objects using for weighing hosts.

In MCP the filter_scheduler is used with Nova default host manager. The following filters are
enabled:

- DifferentHostFilter
- SameHostFilter
- RetryFilter
- AvailabilityZoneFilter
- RamFilter
- CoreFilter
- DiskFilter
- ComputeFilter
- ComputeCapabilitiesFilter
- ImagePropertiesFilter
- ServerGroupAntiAffinityFilter
- ServerGroupAffinityFilter
- PciPassthroughFilter
- NUMATopologyFilter
- AggregateInstanceExtraSpecsFilter

A good description could be found in filter scheduler Nova docs [fn:3].

*** Scheduler logic

The filter scheduler has several phases:

1. Get all available compute nodes. Depending on Nova release it either
   could be a request from DB or the Placement service. In Ocata+ release Placement
   is a part of scheduling to get hosts and put allocations.
2. Filter gotten hosts. The list of hosts go through each filter in the
   order specified in *CONF/filter_scheduler/enabled_filters*. For each host a filter returns either
   true or false.
3. Weigh hosts and sort host according to weights in a descending order.
   In Ocata, the following list of weighers is used:

   - DiskWeigher
   - IoOpsWeigher
   - MetricsWeigher
   - RAMWeigher
   - ServerGroupSoftAffinityWeigher
   - ServerGroupSoftAntiAffinityWeigher

   Each weigher calculates weight for a list of host. After that, weights are normalized basing on
   min and max values. Final weight for host is calculated as sum of (weight * weight_multiplier).
   weight_multiplier is specified in CONF. Default multipliers are the following:

   - ram_weight_multiplier: 1.0
   - disk_weight_multiplier: 1.0
   - io_ops_weight_multiplier: -1.0
   - soft_affinity_weight_multiplier: 1.0
   - soft_anti_affinity_weight_multiplier: 1.0

   A negative value means the logic is reversed, e.g. the more io_ops on a host
   the lower host is on the sorted list.

4. Cut the weighed host list according to the *CONF/filter_scheduler/host_subset_size*
   the default is 1.

5. Pick a random host from weighed host subset.

Let's see some simple cases.

1. Equal hosts scheduling.
   We have several equal hosts, let's assume that filters pass all hosts, in that
   case only weighers affect scheduling. On the first request, a host is picked
   depending on the order in the DB, probably the order of creation.
   On the next requests, the most un-occupied hosts are picked.

2. Big+small hosts scheduling.
   In that case, most powerful hosts are used until their remaining resources
   will be less or equal to the other less powerful hosts.

3. Lay aside scheduling for hosts with big disks.
   In that case, we need to set disk_weight_multiplier to a negative value.
   For example, we have hosts with 4Tb disks (big) and 512Gb disks (small).
   Setting disk_weight_multiplier to -10 allows to schedule on a host with small
   disks first. This assumes that other host params besides the disk space
   are equal.

*** Params affecting scheduling

The ways affect scheduling is mostly coded in filters.
The following params could affect scheduling:

- availability_zone - a param for create server request [fn:4], it restricts the list of
  hosts can be used to the hosts with particular availability_zone;
- scheduler_hints - params for the create server request

  - build_near_host_ip
  - cidr
  - different_cell
  - different_host
  - group
  - query
  - same_host
  - target_cell

- flavor extra_specs

  - pci_passthrough:alias
  - hw:cpu_policy
  - hw:cpu_thread_policy
  - aggregate_instance_extra_specs
  - capabilities:*

- image properties

  - hw:cpu_policy
  - hw:cpu_thread_policy
  - hw_architecture
  - img_hv_type
  - hw_vm_mode
  - img_hv_requested_version

- Placement allocation candidate request options.
  Since Pike+ it is possible to specify params for the Placement request with
  flavor extra specs:

  - resources:$RESOURCE_CLASS:
  - resources$N:$RESOURCE_CLASS:
  - trait:$TRAIT_NAME:
  - trait$N:$TRAIT_NAME:

** Debug scheduling

As there many factors affecting the scheduling process sometimes it
needs to go deeper to understand what happens. The questions
wanted to be answered could be:
- How many hosts were gotten by the scheduler? What are they?
- How many hosts were passed/rejected by each filter? What are they?
- What weight the host have? What value was gotten from the particular weigher?
- What host was chosen and where VM is actually placed?

For that purpose, Mirantis implemented a patch that logs all required data
in JSON-format into a file (/tmp/scheduling).

After booting a VM, scheduler_debug_file could be uploaded into DB
and analyzed with arbitrary SQL-queries.

The following fields are available:
- server - host where the scheduler works
- step - phase of the scheduler process (load, filtering, weighing, subset, random)
- host - hostname of a candidate for booting VM
- weight - aggregated weight value (available on from weighing step only)
- weights - detailed info about particular weighers after normalization
- filter - name of the filter applied
- req - request id
- pid - process id

** Test on Mirantis lab

*** Env description

There is an env with 20 compute nodes.

#+BEGIN_SRC shell :session shell-o20-ctl01 :results raw replace
openstack hypervisor list
#+END_SRC

| ID | Hypervisor Hostname                                   | Hypervisor Type | Host IP       | State |
|  1 | cmp13.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local | QEMU            | 172.16.10.96  | up    |
|  4 | cmp17.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local | QEMU            | 172.16.10.112 | up    |
|  7 | cmp12.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local | QEMU            | 172.16.10.108 | up    |
| 10 | cmp15.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local | QEMU            | 172.16.10.99  | up    |
| 13 | cmp1.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  | QEMU            | 172.16.10.107 | up    |
| 16 | cmp2.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  | QEMU            | 172.16.10.113 | up    |
| 19 | cmp16.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local | QEMU            | 172.16.10.106 | up    |
| 22 | cmp6.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  | QEMU            | 172.16.10.103 | up    |
| 25 | cmp10.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local | QEMU            | 172.16.10.109 | up    |
| 28 | cmp4.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  | QEMU            | 172.16.10.111 | up    |
| 31 | cmp0.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  | QEMU            | 172.16.10.104 | up    |
| 34 | cmp9.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  | QEMU            | 172.16.10.110 | up    |
| 37 | cmp14.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local | QEMU            | 172.16.10.105 | up    |
| 40 | cmp18.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local | QEMU            | 172.16.10.101 | up    |
| 43 | cmp19.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local | QEMU            | 172.16.10.114 | up    |
| 46 | cmp3.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  | QEMU            | 172.16.10.102 | up    |
| 49 | cmp8.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  | QEMU            | 172.16.10.100 | up    |
| 52 | cmp5.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  | QEMU            | 172.16.10.97  | up    |
| 55 | cmp11.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local | QEMU            | 172.16.10.98  | up    |
| 58 | cmp7.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  | QEMU            | 172.16.10.95  | up    |

Nova-scheduler version corresponds to Ocata release.

#+BEGIN_SRC shell :session shell-o20-ctl01 :results replace org
apt policy nova-scheduler
#+END_SRC

#+BEGIN_SRC org
nova-scheduler:
  Installed: 2:15.1.0-2~u16.04+mcp60
  Candidate: 2:15.1.0-3~u16.04+mcp68
  Version table:
     2:15.1.0-3~u16.04+mcp68 1100
       1100 http://mirror.fuel-infra.org/mcp-repos/ocata/xenial ocata/main amd64 Packages
 ,*** 2:15.1.0-2~u16.04+mcp60 100
        100 /var/lib/dpkg/status
     2:13.1.4-0ubuntu4.2 500
        500 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages
     2:13.0.0-0ubuntu2 500
        500 http://archive.ubuntu.com/ubuntu xenial/main amd64 Packages
#+END_SRC

Set scheduler_subset_size to 5.

#+BEGIN_SRC sh :session shell-o20-cfg :results silent
salt ctl* pkg.install crudini --no-color
salt ctl* cmd.run 'crudini --set /etc/nova/nova.conf filter_scheduler host_subset_size 5' --no-color
salt ctl* cmd.run 'systemctl restart nova-scheduler' --no-color
#+END_SRC

*** Apply the patch for the scheduler

Patch scheduler to log inner state (https://review.fuel-infra.org/#/c/38384/).

#+BEGIN_SRC sh :session shell-default :results silent
git format-patch -1 HEAD
scp 0001-Add-debug-logging-for-scheduler.patch o20-cfg:/tmp/
#+END_SRC

#+BEGIN_SRC sh :session shell-o20-cfg :results silent
salt-cp ctl* /tmp/0001-Add-debug-logging-for-scheduler.patch /tmp/0001-Add-debug-logging-for-scheduler.patch
salt ctl* cmd.run 'cd /usr/lib/python2.7/dist-packages/; patch -p1 --backup < /tmp/0001-Add-debug-logging-for-scheduler.patch'
salt ctl* cmd.run 'systemctl restart nova-scheduler' --no-color
#+END_SRC

Clear scheduling log.

#+BEGIN_SRC sh :session shell-o20-cfg :results silent
salt ctl* cmd.run 'rm /tmp/scheduling' --no-color
#+END_SRC

*** Boot VMs

#+BEGIN_SRC shell :session shell-o20-ctl01 :results silent
openstack server list -c ID -f value | xargs openstack server delete
for i in {1..20}; do nova boot --flavor f1 --image 8466bbdb-7bd3-4528-a3a3-a3b1c9ecbb32 --nic none vm-$i; sleep 10; done
#+END_SRC

*** Check VM distribution

#+BEGIN_SRC sh :session shell-o20-ctl01 :results replace org
openstack server list -c Name -f value | wc -l
openstack server list --long -c Host -f value | sort | uniq -c | sed -e 's/^[[:space:]]*//'
#+END_SRC

#+BEGIN_SRC org
20
1 cmp0
1 cmp1
1 cmp10
1 cmp11
1 cmp12
1 cmp13
1 cmp14
1 cmp15
1 cmp16
1 cmp17
1 cmp18
1 cmp19
2 cmp2
1 cmp3
1 cmp4
1 cmp5
1 cmp7
1 cmp8
1 cmp9
#+END_SRC

We see one compute node has got two VMs. That could happen starting from vm-16 as
already chosen nodes were gotten into the host subset. After that, all depends
on the Python random function. That cannot happen if host_subset_size equals 1.

The same distribution should be in the Placement service.

#+BEGIN_SRC sh :session shell-o20-mysql :results replace raw
select p.name, count(*) from allocations a join resource_providers p on a.resource_provider_id = p.id where resource_class_id = 0 group by 1;
#+END_SRC

| name                                                  | count(*) |
| cmp0.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  |        1 |
| cmp1.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  |        1 |
| cmp10.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local |        1 |
| cmp11.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local |        1 |
| cmp12.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local |        1 |
| cmp13.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local |        1 |
| cmp14.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local |        1 |
| cmp15.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local |        1 |
| cmp16.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local |        1 |
| cmp17.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local |        1 |
| cmp18.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local |        1 |
| cmp19.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local |        1 |
| cmp2.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  |        2 |
| cmp3.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  |        1 |
| cmp4.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  |        1 |
| cmp5.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  |        1 |
| cmp7.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  |        1 |
| cmp8.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  |        1 |
| cmp9.vsaienko-deploy-heat-os-ha-ovs-440.bud-mk.local  |        1 |
19 rows in set (0.00 sec)

*** Analysis of what happened

#+BEGIN_SRC sh :session shell-o20-cfg :results silent
salt 'ctl*' cp.get_file_str /tmp/scheduling | grep '{' > /tmp/scheduling
#+END_SRC

#+BEGIN_SRC sh :session shell-default :results silent
scp o20-cfg:/tmp/scheduling /tmp/scheduling
cd /tmp
rm /tmp/scheduling.sqlite
create_db_from_json.py scheduling
#+END_SRC

See the number of requests and created time.

#+BEGIN_SRC sqlite :db /tmp/scheduling.sqlite :results replace
select server, req, host, created from scheduling where step = 'random' order by created;
#+END_SRC

| ctl01 | req-f8ec4d03-6c35-45e1-95f4-51e32d2a3bb3 | cmp11 | 2018-05-04T13:40:56.026566 |
| ctl02 | req-6190dadd-620e-4ddd-9fe7-82cd0b64b51e | cmp15 | 2018-05-04T13:41:08.460139 |
| ctl03 | req-97b43620-f26c-401a-8c62-d8cf5dd19f0e | cmp2  | 2018-05-04T13:41:20.819444 |
| ctl01 | req-30ec51d1-711d-4845-a93d-47fb445ecd2d | cmp19 | 2018-05-04T13:41:33.205596 |
| ctl02 | req-0d71ef5f-767b-42f1-9fe1-fb3ffdbfba21 | cmp9  | 2018-05-04T13:41:45.654428 |
| ctl03 | req-102e9b57-c927-4dd9-8dec-8ea9ef1b530a | cmp4  | 2018-05-04T13:41:58.206847 |
| ctl01 | req-f996fdc7-3510-40df-b504-95f5f94e1af4 | cmp12 | 2018-05-04T13:42:10.908965 |
| ctl02 | req-0d3cc353-d697-4738-b476-7e952022c674 | cmp3  | 2018-05-04T13:42:23.522354 |
| ctl03 | req-b81f3bae-9dc6-45d6-a03a-74113f8bba99 | cmp0  | 2018-05-04T13:42:35.972741 |
| ctl01 | req-5e50c2cf-2a54-4af0-9405-8498ac0068dc | cmp17 | 2018-05-04T13:42:48.485859 |
| ctl02 | req-38ce21ae-abee-4f24-9512-312fa7c2ce1b | cmp5  | 2018-05-04T13:43:00.833606 |
| ctl03 | req-c2405f4c-8373-422f-b856-13bf2ce7fbbc | cmp7  | 2018-05-04T13:43:13.215860 |
| ctl01 | req-a29ea860-6355-400c-b435-d63b0e542c8c | cmp8  | 2018-05-04T13:43:25.698347 |
| ctl02 | req-1358c037-6e37-4ab2-969a-e8d912fe4ff1 | cmp1  | 2018-05-04T13:43:38.098136 |
| ctl03 | req-e9182502-8b2c-4925-830d-6e734a396979 | cmp14 | 2018-05-04T13:43:50.466340 |
| ctl01 | req-f3770fe9-6f11-4573-938f-276929a8aad2 | cmp18 | 2018-05-04T13:44:02.981531 |
| ctl02 | req-d48cd9f1-97bb-4bb9-b0fe-44a533c486b2 | cmp10 | 2018-05-04T13:44:15.350501 |
| ctl03 | req-f095cb43-1c21-4a0f-9d91-54bc6d1f28ff | cmp13 | 2018-05-04T13:44:27.766081 |
| ctl01 | req-a77da227-08dc-4f1e-9d54-b3447619dc14 | cmp2  | 2018-05-04T13:44:40.280627 |
| ctl02 | req-6d2ccc41-ec43-48f5-98f8-4b10ea222f68 | cmp16 | 2018-05-04T13:44:52.640829 |

We can see that cmp2 host was picked on the third and nineteenth requests.

See numbers of hosts for each step for the first request.

#+BEGIN_SRC sqlite :db /tmp/scheduling.sqlite :results replace
select step, filter, count(*) from scheduling where req = 'req-f8ec4d03-6c35-45e1-95f4-51e32d2a3bb3' group by 1, 2 order by created;
#+END_SRC

| load      |                                   | 20 |
| filtering | DifferentHostFilter               | 20 |
| filtering | SameHostFilter                    | 20 |
| filtering | RetryFilter                       | 20 |
| filtering | AvailabilityZoneFilter            | 20 |
| filtering | RamFilter                         | 20 |
| filtering | CoreFilter                        | 20 |
| filtering | DiskFilter                        | 20 |
| filtering | ComputeFilter                     | 20 |
| filtering | ComputeCapabilitiesFilter         | 20 |
| filtering | ImagePropertiesFilter             | 20 |
| filtering | ServerGroupAntiAffinityFilter     | 20 |
| filtering | ServerGroupAffinityFilter         | 20 |
| filtering | PciPassthroughFilter              | 20 |
| filtering | NUMATopologyFilter                | 20 |
| filtering | AggregateInstanceExtraSpecsFilter | 20 |
| weighing  |                                   | 20 |
| subset    |                                   |  5 |
| random    |                                   |  1 |

See weights for the first request. Despite on cmp5 is on the first place cmp11 was finally picked.

#+BEGIN_SRC sqlite :db /tmp/scheduling.sqlite :results replace
select host, weight, weights from scheduling where req = 'req-f8ec4d03-6c35-45e1-95f4-51e32d2a3bb3' and step = 'weighing' order by id;
select host, weight, weights from scheduling where req = 'req-f8ec4d03-6c35-45e1-95f4-51e32d2a3bb3' and step = 'random' order by id;
#+END_SRC

| cmp5  |           2.0 | {u'DiskWeigher': 1.0, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 2.0, u'IoOpsWeigher': 1.0, u'MetricsWeigher': 0.0}                                              |
| cmp11 |           2.0 | {u'DiskWeigher': 1.0, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 2.0, u'IoOpsWeigher': 1.0, u'MetricsWeigher': 0.0}                                              |
| cmp15 |           2.0 | {u'DiskWeigher': 1.0, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 2.0, u'IoOpsWeigher': 1.0, u'MetricsWeigher': 0.0}                                              |
| cmp12 |           2.0 | {u'DiskWeigher': 1.0, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 2.0, u'IoOpsWeigher': 1.0, u'MetricsWeigher': 0.0}                                              |
| cmp14 |           2.0 | {u'DiskWeigher': 1.0, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 2.0, u'IoOpsWeigher': 1.0, u'MetricsWeigher': 0.0}                                              |
| cmp2  | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp8  | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp19 | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp4  | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp9  | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp3  | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp7  | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp0  | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp17 | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp18 | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp1  | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp16 | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp10 | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp6  | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp13 | 1.97222222222 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9722222222222223, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp11 |               |                                                                                                                                                                                                                          |

See weights for the nineteenth request. We see that cmp2 has not the
best weight but it is still in top 5 and it was picked.

#+BEGIN_SRC sqlite :db /tmp/scheduling.sqlite :results replace
select host, weight, weights from scheduling where req = 'req-a77da227-08dc-4f1e-9d54-b3447619dc14' and step = 'weighing' order by id;
select host, weight, weights from scheduling where req = 'req-a77da227-08dc-4f1e-9d54-b3447619dc14' and step = 'random' order by id;
#+END_SRC

| cmp16 |           2.0 | {u'DiskWeigher': 1.0, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 2.0, u'IoOpsWeigher': 1.0, u'MetricsWeigher': 0.0}                                              |
| cmp6  |           2.0 | {u'DiskWeigher': 1.0, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 2.0, u'IoOpsWeigher': 1.0, u'MetricsWeigher': 0.0}                                              |
| cmp5  | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp2  | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp8  | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp11 | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp19 | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp4  | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp9  | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp15 | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp3  | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp12 | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp7  | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp0  | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp17 | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp18 | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp14 | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp1  | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp10 | 1.93795639435 | {u'DiskWeigher': 0.9722222222222222, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.9379563943544669, u'IoOpsWeigher': 0.9722222222222222, u'MetricsWeigher': 0.0} |
| cmp13 | 1.46573417213 | {u'DiskWeigher': 1.0, u'ServerGroupSoftAntiAffinityWeigher': 0.0, u'ServerGroupSoftAffinityWeigher': 0.0, u'RAMWeigher': 1.4657341721322448, u'IoOpsWeigher': 0.5, u'MetricsWeigher': 0.0}                               |
| cmp2  |               |                                                                                                                                                                                                                          |

[fn:1] https://docs.openstack.org/nova/latest/configuration/config.html?highlight=host_manager#scheduler

[fn:2] https://docs.openstack.org/nova/latest/configuration/config.html?highlight=driver#scheduler

[fn:3] https://docs.openstack.org/nova/latest/user/filter-scheduler.html

[fn:4] https://developer.openstack.org/api-ref/compute/#create-server

[fn:5] https://docs.openstack.org/nova/latest/admin/configuration/schedulers.html

[fn:6] https://github.com/amadev/presentations/blob/master/nova_scheduler.org
